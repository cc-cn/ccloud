黑客涉及到的知识非常广泛，主要是web、数据库、linux、python、c++等等
具体参见https://zhuanlan.zhihu.com/p/22304241

caffe框架结构梳理：
利用protobuf存储结构化数据：比如各个层参数
利用blob存储网络过程中的高维数组，比如权重、偏置、神经元
solver.prototxt是存储网络超参数的proto文件，对应Solver类；train_test.prototxt是存储网络结构的proto文件，对应Net类。
每一层都有一个hpp、还有一个cpp or cu文件来刻画。cpp文件对应的层使用cpu来计算，cu文件对应的层使用cuda gpu计算，所以可以能涉及到内存和显存之间的频繁拷贝。以cpp文件为例，每一个 layer 都定义了 3 种重要的运算:setup(初始化设置)，forward(前向传播)， backward(反向传播)。在阅读时，首先根据prototxt文件配置网络超参数，即caffe.proto中相应层的参数（比如type: "Convolution"对应着ConvolutionParameter这个message，相当于为这个类初始化。注意在自己写层的时候，如果需要新的type，还需要改caffe.proto文件，如果只是小改实现细节，则只需要改cpp文件即可。疑问：ConvolutionParameter这个message这个message是怎么和conv_layer.cpp对应上的？？答案：conv_layer.cpp里面定义的类就叫ConvolutionLayer，是以type: "Convolution"作为前缀的，这在自己写层时命名要注意。）。其次注意父类中的变量及函数的含义，最后以bottom、this_blobs_、top为线索阅读。不需改变的量，采用const类型。养成指针运算的习惯，非常强大！！
caffe提供命令行接口、python接口、matlab接口。分别在caffe/build/tools、caffe/examples、caffe/matlab，具体用法参见官方文档。
在caffe的prototxt可以调用自定义的python layer，
例如https://github.com/rbgirshick/py-faster-rcnn/blob/master/models/pascal_voc/VGG16/faster_rcnn_end2end/train.prototxt
调用了https://github.com/rbgirshick/py-faster-rcnn/blob/master/lib/rpn/proposal_layer.py（训练的时候使用nms）


pytorch计算图是靠Variable实现得到的,普通计算是靠tensor实现.Variable可自动计算梯度.

jupyter-notebook更改变量值之后,前面出现的变量值也相应更改.

caffe LayerParameter类定义在:.build_release/src/caffe/proto/caffe.pb.h




 



如何用jupyter打开远程服务器上的文件？
conda安装jupyter：conda install jupyter
登录远程服务器：ssh -L localhost:8888:localhost:8888 chencheng@192.168.3.111
获取网址，在本地浏览器打开即可：jupyter notebook

查看目录下文件个数：ls -l|grep "^-"|wc -l
查看目录及其子目录下文件个数：ls -lR|grep "^-"|wc -l
grep "^-"的含义是搜索以"-"开头的行的个数

与人交流的文件一定要用大众的格式，比如txt、doc、ppt，否则可能会出现打不开的问题。这是常识、也是礼貌。

clear caches
sudo su -c "sync; echo 1 > /proc/sys/vm/drop_caches"

I0122 14:54:16.620296 16591 blocking_queue.cpp:50] Waiting for data：训练的时候老是出现这句话，猜测是多batch的时候IO读取跟不上导致的。可以把要调用的数据放到ssd上。不用之后再从ssd删掉。

在hus——tools里找到一个脚本lmdb_lib.py,可以concat两个lmdb文件。
顿时感觉lmdb不再可怕。提高了效率。//额，训练的时候发现这样生成的lmdb无法训练。。提示数据大小为0.。而且还要手动生成其它txt文件，不够麻烦的。

sudo -s进入root用户
exit 回到上一个用户

bug：Check failed: state.history_size() == history_.size() (63 vs. 79) Incorrect length of history blobs.
解决：更改了网络后不要finetune，而是pretrain。（solverstate vs caffemodel）

查看测试loss，看看是否过拟合：测试的时候会输出SoftmaxwithLoss1，这个loss大致能反映出测试效果。如果训练loss在下降，这个loss反而上升，且远大于训练loss，那很可能是出现了过拟合。

追踪detection_evaluate_layer.cpp中的resize_param_含义，哪里用到了？和prototxt中的resize区别？
/home/chencheng/server2/src/caffe/util/bbox_util.cpp
detection_output首先得到比例，再利用原图的size得到原图框的实际位置。而val.lmdb存的label是resize之后的label，故还需要真实size得到真实图片上的gt框。由此计算mAP。

py-faster-rcnn训练自己的数据，如何自定义网络？并生成需要的prototxt文件？//net.to_proto()函数

mAP计算方法：mxnet-ssd里113:~/workspace/mxnet-ssd/jobs/pascal_voc/vgg16_reduced/myeval_voc.py   过阵子再看看这段代码

对照ssd.py搭建网络的脚本以及prototxt文件。
in_place：猜测是使用同一个变量名，节省存储空间。bottom，top同名。但在net.keys()可以不同名。
我在202：/home/chencheng/workspace/caffe/jobs/person_car/get_prototxt写了一个脚本，可以将搭建网络的python代码转化为prototxt文件，这样再配合netscope就很直观了。比如resnet50，101，152系列，block都是5，每个block都有一个尺寸减半的操作。ssd的outputlay层通常是从block连接出选取。不同的是第三、第四个block里有一个循环（循环次数不同），每个循环做一次ResBody（stride=1）.

试验一下：exp(x)=(1+x/n)^n 
精度：exp好
时间：python语言exp快一倍；c++语言(1+x/n)^n快一倍；

过拟合：训练loss在下降，测试loss在上升。（网络大，训练数据少容易过拟合）

debug是很有方法的。一个大的工程出现了bug，可以先回到上一个没有bug的版本，然后对比改动的地方，一点一点改，看看是哪一个改动造成了bug.再就是可以使用中间输出，看看bug开始的地方。以及环境路径是否正确。

103服务器上的环境比较纯净，可以作为新建框架的范本，直接从103拷贝即可。

通过.sh安装的anaconda文件，安装好之后，这个安装好的文件夹不能移动路径，因为很多路径依赖在安装的时候就已经确定下来了

志成说了一中可以不在环境变量中添加caffe路径的方法，就是在caffe.cpp中添加环境变量！

vim可以实现按列排序
:1,598!sort -r -n -k4.1,5表示将1至５９８行安装第４个字段的前五个数字从大到校排序

查看文件的md5码（文件的唯一标识）
md5sum fille
ls | xargs md5sum 

可以顺序查看文件夹下的图片： eog JPEGImages/   

查看软件安装位置：whereis cuda， whereis bash, whereis numpy, whereis ls等等

查看单个文件绝对路径：readlink -f  file 
查看子文件绝对路径：readlink -f file/* 

c语言
#include <stdio.h>
printf ("Characters: %d \n", 19);

删除用户: userdel chencheng
建立用户: adduser chencheng
将用户加入root组：usermod -aG sudo chencheng
切换用户: su - chencheng

nohup ./run.sh >logs/small_log 2>&1&   实现将训练输出写入logs/small_log文件。
ps命令可查看正在终端运行的项目，kill -9 pid可杀掉相应pid。。

find '*.h5' > h5_list
cat train_img_label_size  | awk '{print $1,$2,$3,$4}' > a

shell截取字符串：
echo ${var%/*}
%/* 表示从右边开始，删除第一个 / 号及右边的字符

%s/^.\{4\}//g
将当前缓冲区的所有行的前4个字符删除 ，最后的/g表示全部替换，没有的话只表示每行第一次目标出现时替换。

ls > file_list
ls | head > file_list
vim下替换：%s/old/new

查看cpu温度:https://www.linuxidc.com/Linux/2015-06/119201.htm
查看cpu占用的命令：top    # htop命令比top命令更强大，可以看到每个线程占用、还有内存占用。# iotop可以看磁盘占用
查看cpu总线程数：cat /proc/cpuinfo| grep "processor"| wc -l
查看内存使用情况 free -h
查看硬盘信息 lsscsi，du -sh *，df -h
显示显卡占用，自动0.1秒刷新一次： watch -n 0.1 nvidia-smi 
或者：watch -c -n 0.2 gpustat --color（需要先安装gpustat：pip install gpustat）

在shell脚本中，$@：表示所有脚本参数的内容，$#:表示返回所有脚本参数的个数。

今天在１９２服务器出了一个巨大bug! /mnt下面有个data路径，我在里面放东西导致根目录满了，没法运行。志成帮我找到了原因，原来data并不是挂在外接硬盘上的，而是在根目录下。可以用df -h命令看到哪些地方是挂载的！！不是计算机背景真是硬伤，还要学习！！

Check failed: error == cudaSuccess (74 vs. 0)  misaligned address
debug:cudnn版本问题，不太好搞。。对某些resize会出现这个问题。把900x600改成896x600就好了。

建立软连接：ln -s src绝对路径 目标文件绝对路径   #这个目标文件不需要事先新建好。
查看文件是不是软连接：ls -l

shell常用正则表达式总结：
cat:最简单，用于显示文件内容，或cat test test1 > test2合并两个文件。
grep:关键字搜索，-n参数可现实搜索的行号。
sed：用于对行做处理。sed '1,2d' ab #删除第一行到第二行；sed -n '1,2p' ab #显示第一行到第二行；sed -n '/ruby/p' ab  #查询包括关键字ruby所在所有行

awk（重点）：将一行分为多个字段。awk [-F  field-separator]  'commands'  input-file(s)
其中，commands 是真正awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。
在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。
例子：
cat a.txt | awk  '{print $1}'  #取出空格分开的第一个字段，，$0则表示所有域
cat /etc/passwd |awk  -F ':'  '{print $1}'   #-F 指定分隔符为 ：
cat /etc/passwd |awk  -F ':'  '{print $1"\t"$7}'    #显示第1、7个字段
#搜索/etc/passwd有root关键字的所有行，并显示对应的shell
awk -F: '/root/{print $7}' /etc/passwd             

dav视频抽取为png图片https://jingyan.baidu.com/article/15622f243a995afdfcbea521.html
ffmpeg -i 10.209.2.59-0-201709301900-201709302200.dav -r 0.01 avi-img/cc-%3d.jpeg（截取视频中img，每秒0.01张）

ls一个具有50w张图片的文件夹，并将文件名重定向到list需要的时间：~60s

caffe.proto文件给出了protobuf的数据结构，可以轻易转化为cpp类，也可转化为java，python调用的类。在caffe/layers, 许多参数的含义都源自于这里。

今天遇到了一个conv4_3没加normalization导致训练效果很差的现象。问了志成，他猜测是较底层在反向传播时容易出现梯度弥散。加一个值为20的normalization后，5000迭代mAP从0.7%变为30%.一个参数竟然影响这么大！！

终端可以import tensorflow,而pycharm不能的解决办法：把pycharm--preference-interpretor改成tensorflow库所在的python。

这个命令可以近似查看训练的各部分占用时间,测试时各层占用时间；之所以是近似是因为网络的输入图片是随机的初始化的，所以score比较低，这减少了nms的时间。这种方式并不需要数据，只需要prototxt文件。
~/workspace/caffe/build/tools/caffe time --model train.prototxt  -gpu 1 --iterations 100 1>log 2>&1
~/workspace/caffe/build/tools/caffe time --model deploy.prototxt  -gpu 1 --iterations 100 1>log 2>&1,这个时候观察显存占用，还能看到模型训练和测试时需要的显存。这在嵌入式中是重要参数。

python中的浅拷贝，深拷贝：http://www.cnblogs.com/zxlovenet/p/4575228.html
list等对象的复制是至少是指针的赋值，最浅。
copy.copy()是浅拷贝，只拷贝最外层，对于子对象，相当于复制。
copy.deepcopy()是深拷贝，所以的内存都是新的。得到两个完全独立的对象。


工作前的准备：查看硬件占用（硬盘，显卡，cpu）；规划好命名规则，不要太模糊，要不然过段时间再看就不记得了。及时做整理，删掉垃圾文件，写readme文档，整理文件路径。

查看显存被哪个用户占用：top可以看到pid对应的用户，nvidia-smi可以看到pid和显存的对应关系。
但这个办法对只占显存，不占cpu，和显卡算力的情形无效。

vim退格键无法删除的问题：在vimrc里加入两行
set nocompatible 
set backspace=indent,eol,start 
后来又加一些mac下vimrc的配置，更加好用了。看来这些东西，只要认真准备一份就可以了，不必每次都重头来。这也是计算机的美妙之处！

python定义简单函数：
方法1：
f=(lambda x=1, y=2017: x+y)
>>> f()
2018
>>> f(2,4)
6
方法2：
def f(x=1,y=2017):
 return x+y


刚刚开始用公司的台式机的时候遇到需要root密码的问题，我当时已经将自己的加了root权限，但不知道root、密码。当时其实并没有设置root密码，在刘灏的帮助下，输入sudo passwd root即可为root设置密码。

plt.plot([1,2,3,4],[1,4,9,16],'ro')
第一个列表为x轴数据，第二个列表为yz轴数据，第三个代表的画的为点图，'ro'代表是红色的点，如果没有第三个参数的话画出的为线
设置X 轴的范围为0-6，Y轴范围为0-20  plt.axis([0, 6, 0, 20])
清除原有的图层plt.plot(hold=False)
设置可以显示plt.hold(True)
保存图片plt.savefig('.png' % index, format='png')
设置标题plt.title(’Plot of y vs. x’)
X轴的标题plt.xlabel(’x axis’)
Y轴的标题plt.ylabel(’y axis’)
显示图片plt.show()

在训练模型的时候，尽量给GPU 0.5G以上的显存，因为训练期间可能会有test，会占用一些显存。此时就可能是训练断掉。还有就是test数据量尽量在1w左右比较好。第一不会占用过多测试时间，其次这个数据量也比较有说服力。显存紧张的话，test的batch_size选1就可以了。

Python 调用 shell 脚本： 
import subprocess
subprocess.call(str, shell=True)
str可以是shell脚本路径，或者是一条具体的shell命令。

shell调用python脚本非常简单：
python file.py即可
c++调用python则比较麻烦，   http://www.cnblogs.com/jason2004/p/6182988.html

str.isdigit() 判断字符串是否是由纯数字构成。如果是float则比较麻烦，不知道是否有好办法？




sys.exit()通常用在主程序
os._exit(0)通常用在子程序中
shell脚本退出直接 exit n    #n=0表示成功退出。
parse.add_argument("- -size", dest="mysize")  (不加 - -表示必选参数，dest作用是在赋予一个名字，可以用args.mysize访问该参数。可选参数要有default值)
awk '{print NF}' 得到列数
awk '{print $NF}' 得到最后一列的信息

check_if_exist(filefolder)
make_if_not_exist(filefolder)

os.environ['HOME'] 返回当前用户home绝对路径
os.getcwd()返回当前工作目录绝对路径。

判断字典中是否含有某个元素 if key in dict:

python 教程：https://www.cnblogs.com/kaituorensheng/p/4465768.html

当改变网络某一层的channel的时候，使用pretrain要注意，因为导入pretrain参数是根据层的名称的，所以要换一个新的名字。

pandas模块:
pd.DataFrame

softmax损失：   https://zhuanlan.zhihu.com/p/21102293?refer=intelligentunit 其实就是交叉熵

交叉熵：交叉熵可在神经网络(机器学习)中作为损失函数，p表示真实标记的分布，q则为训练后的模型的预测标记分布，交叉熵损失函数可以衡量p与q的相似性。交叉熵作为损失函数还有一个好处是使用sigmoid函数在梯度下降时能避免均方误差损失函数学习速率降低的问题，因为学习速率可以被输出的误差所控制。

c++源文件和头文件include问题，需要makefile文件，就是设定哪些是要编译的cpp文件。编译的结果是建立cpp文件和头文件的联系，生成可执行文件。

c++中二维数组传参是危险的，尽量转化为一维（可采用延展的办法，或者适应多个一维数组替代。）

macOS升级后jupyter notebook报错解决:   http://www.jianshu.com/p/be11a6edb8a6

plt.subplot(221) //分成2x2，占用第一个，即第一行第一列的子图  
plt.subplot(222)//分成2x2，占用第二个，即第一行第二列的子图  
plt.subplot(212)//分成2x1，占用第二个，即第二行

利用Anaconda安装python后，想要安装opencv-python，但发现利用opencv-python的官方教程，没法实现opencv的安装
还好看到了另外一篇博客的方法，试一下，果然凑效
即：直接在cmd命令行输入：conda install --channel   https://conda.anaconda.org/menpo opencv3
然后，根据提示输入y即可

python 接口设置是否用GPU：
if args.cpu_mode:
 caffe.set_mode_cpu()
else:
 caffe.set_mode_gpu()
 caffe.set_device(args.gpu_id)
 cfg.GPU_ID = args.gpu_id

c++接口设置是否用GPU：
if (use_GPU) {
 Caffe::SetDevice(gpu_id);
 Caffe::set_mode(Caffe::GPU);
 useGPU_ = true;
 }
 else {
 Caffe::set_mode(Caffe::CPU);
 useGPU_ = false;
 }

C++  atoi(str):将str转化为int

用vs code打开文件的时候，设置不显示隐藏文件：code-首选项-设置，在 settings.json 中进行如下设置：
// 配置 glob 模式以排除文件和文件夹。  
"files.exclude": {  
 "**/.*": true,  // 排除 隐藏 文件
 "**/*.pyc": true   // 排除 Pycharm 的配置文件
}

mac安装brew，直接google：mac install brew即可。
以前百度各种搜都没成功。难道是谷歌大法好？也可能是是环境变了（如网络，重启，等）在一个问题上卡太久之后，不妨先放一放。说不定哪天就好了。

rfcn训练的样本选择策略：online hard example mining (OHEM) 。主要思想就是对样本按loss进行排序，选择前面loss较小的，这个策略主要用来对负样本进行筛选，使得正负样本更加平衡。

R-FCN是在Faster R-CNN的框架上进行改造，第一，把base的VGG16换车了ResNet，第二，把Fast R-CNN换成了先用卷积做prediction，再进行ROI pooling。由于ROI pooling会丢失位置信息，故在pooling前加入位置信息，即指定不同score map是负责检测目标的不同位置。pooling后把不同位置得到的score map进行组合就能复现原来的位置信息。

64位编译器
char ：1个字节
char*(即指针变量): 8个字节
short int : 2个字节
int：  4个字节
unsigned int : 4个字节
float:  4个字节
double:   8个字节
long:   8个字节
long long:  8个字节
unsigned long:  8个字节 

~/data下尽量只放原始数据， ~/框架/data下可以放test小数据，img_label,缓存等。
今天误删除了huyy挂载过来的yingshi数据，差点犯了大错。以后删除东西一定要谨慎。rm * 这类命令不要用了，太危险。
挂载过来的东西不用了就 umount file

python判断文件和文件夹是否存在、创建文件夹
import os
os.path.exists('dir')
os.path.isfile('dir')
os.makedirs('dir')

vim下多行缩进：esc ， V （选中一行，再按上下可多行）， < (向左缩进)

>>> a = [1,2,3]
>>> b = [4,5,6]
>>> zipped = zip(a,b)
[(1, 4), (2, 5), (3, 6)]

r表是读 (Read) 、w表示写 (Write) 、x表示执行 (eXecute)
读、写、运行三项权限可以用数字表示，就是r=4,w=2,x=1，777就是rwxrwxrwx，意思是该登录用户（可以用命令id查看）、他所在的组和其他人都有最高权限。
例子：
chmod 777 file 对所有用户都有读写执行权限
chmod +x file 增加执行权限
chmod -x file 去掉执行权限
chmod 664 file 登录用户具有读写权限、登录用户所在组具有读写权限、 其他组具有只读权限。  这是最常见的权限类型（一般自己新建的文件默认使用这种权限类型）

在设置anchorbox的时候可以先统计一下groundtruth的分布。试试这个想法在枪支上的效果。

长时间训练可考虑用nohup以防突发情况。或至少要存caffemodel，snapstate，log训练日志。
磁盘上，数据的存储是线性的。要想表达一个二维（或更高维）的矩阵，引入了符号"["   ，"]"。底层的调用还是一维的。
跑大批量数据的时候可以输出个进度提示，比如以百为单位输出。

重定向，正则表达式，管道是个好东西。

追加写文件output = open('data.txt', 'a')  
output .write("\n都有是好人")  
output .close( )  
写数据file_object = open('thefile.txt', 'w')  
file_object.write(all_the_text)  
file_object.close( )

pandas在数据预处理上很好用。
测试自己编写的cpp：1. 一般例子，2.边界例子，3。某一类例子（可以专门编一个程序）。
pycharm: command+选中函数  可现实函数定义
c++也有类似功能，在vs code

C++中常类型是指使用类型修饰符const说明的类型，常类型的变量或对象的值是不能被更新的，使用const将大大改善程序的健壮性。两点建议：1. 任何不会修改数据成员的函数都应该声明为const 类型。2. new返回的指针必须是const类型的。

一个好的程序应该有自己纠错的能力，在可能有错的地方设置输出提示。比如index超出范围，分母为0。

PR曲线用来反映检测效果，即查准率（Precision）与查全率（Recall），以查全率为坐标x轴，查准率为坐标y轴，从而画出了一条曲线。按照某阈值 预测出来的框中，Precision=正确的个数占预测总框个数的比例；Recall=正确的个数占实际总框个数的比例。AP就是某个类的PR曲线与坐标轴围成的面积，mAP就是各个类对应AP的评价。

Roc曲线——而分类问题中（有病没病）根据预测所采用的阈值不同可以画出一系列的二维点，横坐标是false positive rate(FPR，错误预测为真的样本个数除以实际为假的样本个数)，纵坐标是true positive rate(TPR，成功预测为真的样本个数除以实际为真的样本个数)。这个曲线一定过（0，0），（1，1），且通常在两点连线的上方
AUC的值就是处于ROC curve下方的那部分面积的大小。用来综合反映一个分类器的好坏
参见https://www.cnblogs.com/gatherstars/p/6084696.html



ipad可以使用：teamviewer
也可以连服务器：termius,最好有个外接键盘
昌伟有一个ip端口可以直接远程公司服务器。

经测试，alt+VGG16效果优于alt+ZF。


./experiments/scripts/faster_rcnn_alt_opt.sh [GPU_ID] [NET] [--set ...]
./experiments/scripts/faster_rcnn_end2end.sh [GPU_ID] [NET] [--set ...]
NET可以选ZF，VGG16, VGG_CNN_M_1024 三种，第三个参数是数据集，可选coco，pascal_voc

现在大量小网络中练习debug，调参数，编码的能力！！
TypeError: slice indices must be integers or None or have an __index__ method？？？？没有不降numpy版本的方法还是没有解决。晶晶按照网上方法解决了，估计是我这边其他配置的问题。。唉，耽误了三个小时。将ZF换成VGG网络就可行了。神奇

vim里面tab相当于8个空格，可是pycharm里面相当于4个空格。所以在修改文件的时候有的地方应该用空格的，却用成了tab，虽然看起来一样，但运行就会出错！！血的经验啊！最好在pycharm里编辑文件吧！

python h5py可以读h5文件。
仔细看看rpn网络源码。看懂这个学其他检测方法就快了。

软连接和硬连接都只有一个文件。软连接相当于创建快捷方式。硬连接不同的地方在于删除所有文件才会让被连接的文件消失。


本地sever可以更改服务器文件，但服务器下不可调用server路径文件，别搞混淆了。
可以现在小数据集上调试，节省时间。
lr一开始设置大一点，0.01，0.001
cache文件名自定义，会生成的。
调参数：深刻理解每个参数的意义，多多训练积累经验。

linux下挂载服务器的方法
安装sshfs：sudo apt-get install sshfs
创建挂载：sshfs chencheng@192.168.3.199:/home/chencheng/ /home/deepglint/server
解除挂载：fusermount -u 挂载点 or umount 挂载点
server是新建的本地空文件夹。
每次开机都要建立一次吗？yes
在家能挂吗？no

pip install easydict

学习的时候可以在pycharm上，便于查看各种函数。
pycharm, 浏览器上面都可以装vim插件。几乎可以摆脱鼠标。

python文件file1在import file2的时候，在在file2所在的文件夹建立一个空的文本文件__init__.py。import可采用file2的相对路径或绝对路径。路径之间用点分隔，无需加.py后缀。详解   http://blog.csdn.net/hansel/article/details/8975663

宏定义的优点：方便程序的修改，减少系统开销，提高运行效率

深度学习中经常看到epoch、 iteration和batchsize，下面按自己的理解说说这三个的区别：
（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；

在当前路径下搜索文本'kitty'：grep -H -R 'kitty'

vim命令大全
i 在当前位置生前插入
I 在当前行首插入，0（数字0）移动到本行第一个字符上
A 在当前行尾插入
?text　查找text，反向查找，按n健查找下一个，按N健查找前一个。
h 左移一个字符
l 右移一个字符，这个命令很少用，一般用w代替。
k 上移一个字符
j 下移一个字符
以上四个命令可以配合数字使用，比如20j就是向下移动20行，5h就是向左移动5个字符，在Vim中，很多命令都可以配合数字使用，比如删除10个字符10x
w 向后移动一个单词
b 向前移动一个单词 2b 向前移动2个单词
gg 移动到文件头，G移动到文件尾。
Ctrl + d 向下滚动半屏
Ctrl + u 向上滚动半屏
Ctrl + f 向下滚动一屏
Ctrl + b 向上滚动一屏
u 撤销（undo）
Ctrl + r 重做（Redo），即撤销的撤销。
x 删除当前字符
3x 删除当前光标开始向后三个字符
10dd 删除当前行开始的10行。

如果操作较多，建议用gedit打开文件。

shell编程
file somefile: 得到文件类型 
head file: 打印文本文件开头几行 
tail file : 打印文本文件末尾几行 
管道   |  将一个命令的输出作为另外一个命令的输入。 
grep "hello" file.txt | wc -l 
在file.txt中搜索包含有”hello”的行并计算其行数。
shell脚本可以看着是命令的集合，$1表示其传入的第一个参数。该变量包含了传递给该程序的第一个参数值。
学会使用echo命令输出中间变量来debug。


随机梯度下降所体现的随机在哪里？batch-epoch-

用ftp在本地和公司服务器之间传文件

../../build/tools/caffe train -solver=solver.prototxt -gpu=0

opencv是一个图像处理库，只是其中封装了传统的机器学习方法和特征提取方式。
深度学习是新兴起的机器学习算法，是神经网络算法的扩展，不再需要人工去提取特征，效果也非常好。
cnn，dnn这些才是算法，caffe，theano这些只是深度学习框架，封装了底层实现，使用者只需要调节参数，降低了深度学习的门槛。

为了用python实现高效的数值计算，我们通常会使用函数库，比如NumPy，会把类似矩阵乘法这样的复杂运算使用 其他外部语言实现。不幸的是，从外部计算切换回Python的每一个操作，仍然是一个很大的开销。如果你用GPU来 进行外部计算，这样的开销会更大。用分布式的计算方式，也会花费更多的资源用来传输数据。 
TensorFlow也把复杂的计算放在python之外完成，但是为了避免前面说的那些开销，它做了进一步完善。Tensorf low不单独地运行单一的复杂计算，而是让我们可以先用图描述一系列可交互的计算操作，然后全部一起在Python 之外运行。(这样类似的运行方式，可以在不少的机器学习库中看到。) 


mac终端
编译运行c++： 
g++  -o test filename.cpp 
./test
编译运行c： 
gcc  -o test filename.c
./test


linux终端
编译运行c++： 
g++  -o test filename.cpp    或者c++  -o test filename.cpp 
./test
编译运行c： 
gcc  -o test filename.c
./test


python显示中文# -*- coding:UTF-8 -*-

caffe GPU方法，cuda，cudnn
caffe/Makefile/config可以修改加速模式
普通：cpu
安装GPU驱动和cuda：可使用GPU模式
再安装cudnn：可使用cudnn加速模式
注意上述需要安装到项目与caffe版本兼容性。

