今日目标：p25，完成
SGD, learning rate, 局部最优解？如何避免呢？



神经网络研究的第二次浪潮一直持续到上世纪 90 年代中期。基于神经网络和其
他AI技术的创业公司开始寻求投资,其做法野心勃勃但不切实际。当AI研究不能实
现这些不合理的期望时,投资者感到失望。同时,机器学习的其他领域取得了进步。
比如,核方法 (Boser et al., 1992; Cortes and Vapnik, 1995; Schölkopf et al., 1999)
和图模型 (Jordan, 1998) 都在很多重要任务上实现了很好的效果。这两个因素导致
了神经网络热潮的第二次衰退,并一直持续到 2007 年。


今日目标：p47，

每个实对称矩阵都可以分解成实特征向量和实特征值:A = QΛQ'
其中 Q 是 A 的特征向量组成的正交矩阵,Λ 是对角矩阵。特征值 Λ[i,i],i 对应的特征
向量是矩阵 Q 的第 i 列,记作 Q[:,i] 。因为 Q 是正交矩阵,我们可以将 A 看作沿方
向 v (i) 延展Λ[i,i] 倍的空间。

奇异值分解(singular value decomposition, SVD):A = UDV'
A 是一个 m × n 的矩阵,那么 U 是一个 m × m 的正交矩阵,D 是一个 m × n
的对角矩阵,V 是一个 n × n 正交矩阵。
对角矩阵 D 对角线上的元素被称为矩阵 A 的 奇异值(singular value)。矩阵
U 的列向量被称为 左奇异向量(left singular vector)
,矩阵 V 的列向量被称 右奇异
向量(right singular vector)。
事实上,我们可以用与 A 相关的特征分解去解释 A 的奇异值分解。A 的 左奇
异向量(left singular vector)是 AA' 的特征向量。 A 的 右奇异向量(right singular
vector)是 A'A 的特征向量。A 的非零奇异值是 A'A 特征值的平方根,同时也是
AA'特征值的平方根。

