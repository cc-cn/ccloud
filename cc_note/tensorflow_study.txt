安装
参考
https://www.tensorflow.org/versions/r1.4/install/install_linux?hl=zh-cn#InstallingAnaconda
因为我的cuda版本为8，所以不能安装最新tensorflow，我选择装1.4
使用conda安装
sudo apt-get install libcupti-dev

conda create -n tensorflow python=3.6
source activate tensorflow
pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.3.0-cp36-cp36m-linux_x86_64.whl

pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.3.0-cp36-cp36m-linux_x86_64.whl

出现了找不到libcudnn.so.6的问题，因为我安装的是cudnn5，所以重新下载安装cudnn6.见我的caffe安装笔记


运行的时候会出现警告：The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations

这是说cpu可以加速，但我用GPU版本，所以可以下述代码忽略警告

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'



参考官方文档快速开始 和 Images部分：https://www.tensorflow.org/tutorials/layers


tensorflow中的赋值、加减乘除等运算都是形式的，并没有真正执行，所以也不占用计算资源，当然也不能输出结算结果。直到
sess=tf.Session()
sess.run(a+b)
这样才是真正的执行。
sess使用完后需要用sess.close()来关闭以释放资源。

使用第二块GPU
with tf.device("/gpu:1"):
	cmd
	cmd
	……


tensorflow是python的一个库，里面有各种深度学习有关的函数，比如卷积，池化，权重初始化等等；这点类似于caffe的python接口。


卷积参数 padding="same" 表示卷积不改变feature大小

训练和测试mnist：  /home/chencheng/shell/tf_study.py，其实就是官方源码tensorflow/tensorflow/examples/tutorials/layers/cnn_mnist.py

官方文档训练结果：{'accuracy': 0.96939999, 'loss': 0.10405618, 'global_step': 20000}




对网络做如下修改：
去掉dropbox层，lr策略？，减小全连接层1024-->256
结果：lr改为0.01可以大大提高acc

疑惑：训练显存居然占到了10G，即便是做了上述精简网络之后显存还是没有改变！！why？
Ans:为了提高计算效率，tf会默认调用所有能调用的资源。如果按需分配，可以添加如下代码
import os
os.environ["CUDA_VISIBLE_DEVICES"] = '0'   #指定第一块GPU可用
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.5  # 程序最多只能占用指定gpu50%的显存
config.gpu_options.allow_growth = True      #程序按需申请内存
sess = tf.Session(config = config)

查看各层占用显存?

TensorFlow 读取数据方法：详见https://blog.csdn.net/liushuikong/article/details/79213632
制作和读取TFRecords文件：https://blog.csdn.net/wiinter_fdd/article/details/72835939
1、使用placeholder和feed_dict读内存中的数据
2、使用queue pipeline(队列式管道)读取硬盘中的数据（原理介绍可以参考这篇文章：十图详解tensorflow数据读取机制）
3、Dataset API是从 TensorFlow 1.3开始添加新的输入管道。使用此 API 的性能要比使用 feed_dict 或队列式管道的性能高得多，而且此 API 更简洁，使用起来更容易。在TensorFlow 1.3中，Dataset API是放在contrib包中的：tf.contrib.data.Dataset，而在TensorFlow 1.4中则是tf.data.Dataset。实现了从内存或者硬盘文件中加载数据组成数据集，同时对数据集进行一系列变换操作，最终将数据集提供给其他API使用的一系列功能。

可视化？tf board
tensorboard --logdir=dir_to_events_file  #这个文件是通过tf.summary.FileWriter生成的


生成三个文件，含义如下：
meta file: describes the saved graph structure, includes GraphDef, SaverDef, and so on; then apply tf.train.import_meta_graph('/tmp/model.ckpt.meta'), will restore Saver and Graph.
index file: it is a string-string immutable table(tensorflow::table::Table). Each key is a name of a tensor and its value is a serialized BundleEntryProto. Each BundleEntryProto describes the metadata of a tensor: which of the "data" files contains the content of a tensor, the offset into that file, checksum, some auxiliary data, etc.
data file: it is TensorBundle collection, save the values of all variables.


使用pretrainmodel，finetune：
with tf.Session() as sess:
	#saver = tf.train.Saver()
	#sess.run(tf.global_variables_initializer())##这是普通的随机初始化

	saver = tf.train.Saver()
	ckpt = tf.train.get_checkpoint_state(pretrain_logs_dir)##放置pretrain model文件夹，或者直接令ckpt=dir_to_pretrained_model
	if ckpt and ckpt.model_checkpoint_path:
		#global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]
		saver.restore(sess, ckpt.model_checkpoint_path)
	else:
		print('no checkpoint')
		sys.exit()

通过meta文件载入网络
with tf.Session() as sess:    
    saver = tf.train.import_meta_graph('my-model-1000.meta')
    saver.restore(sess,tf.train.latest_checkpoint('./'))
    print(sess.run('w1:0'))##打印权重参数


迁移学习，只训练最后一层
saver = tf.train.import_meta_graph('vgg.meta')
# Access the graph
graph = tf.get_default_graph()
## Prepare the feed_dict for feeding data for fine-tuning 
 #Access the appropriate output for fine-tuning
fc7= graph.get_tensor_by_name('fc7:0')
 #use this if you only want to change gradients of the last layer
fc7 = tf.stop_gradient(fc7) # It's an identity function，，打印feature map
fc7_shape= fc7.get_shape().as_list()
new_outputs=2
weights = tf.Variable(tf.truncated_normal([fc7_shape[3], num_outputs], stddev=0.05))
biases = tf.Variable(tf.constant(0.05, shape=[num_outputs]))
output = tf.matmul(fc7, weights) + biases
pred = tf.nn.softmax(output)



更改lr策略：optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)





